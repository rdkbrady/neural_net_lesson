{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install tensorflow_datasets\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Neural Networks\n",
    "\n",
    "Learning objectives:\n",
    "1. Understand what neural networks can do.\n",
    "2. Understand the structure of a neural network.\n",
    "3. Understand forward propagation and backpropagation.\n",
    "4. Understand the structure of a neuron, and be able to select appropriate activation functions.\n",
    "5. Implement a neural network in keras\n",
    "\n",
    "Prerequisites:\n",
    "1. derivatives, including partial derivatives\n",
    "2. gradient descent\n",
    "3. chain rule\n",
    "\n",
    "Some very good videos from 3 blue 1 brown:\n",
    "- [Neural Network playlist (4 videos)](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Applications of Neural Networks\n",
    "\n",
    "\n",
    "### Text Generation\n",
    "The [GPT-2](http://jalammar.github.io/illustrated-gpt2/) model is a **decoder** model which uses previous words to predict the next one in a sequence. You can experiment with a limited version of this model at [talk to transformer](https://talktotransformer.com/). \n",
    "\n",
    "**Input:** In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    "**Output:** The amazing find was made by researchers of the Universidad Austral de Chile, thanks to one of the country's most highly active amateur scientists. Of course, the creature in question was not a real unicorn, but a llama, and unfortunately the strange creature was killed before researchers could analyze its vocalization.\n",
    "\n",
    "\n",
    "### Image Classification\n",
    "The [VGG-16](https://neurohive.io/en/popular-networks/vgg16/) model is a **deep convolutional** network that can classify image regardless of the scale or rotation of the objects in the image.\n",
    "\n",
    "doge? | doge?\n",
    "- | - \n",
    "[<img src=\"img/shibe.jpg\" width=240 />](https://knowyourmeme.com/memes/doge) |[<img src=\"img/doge.jpg\" width=240 />](https://en.wikipedia.org/wiki/Doge_of_Venice)\n",
    "\n",
    "\n",
    "### Image Generation\n",
    "[Generative adversarial networks](https://neurohive.io/en/news/deepfake-videos-gan-sythesizes-a-video-from-a-single-photo/]) can be used to create plausible images and video.\n",
    "<img src=\"img/gan.png\" width=600 />\n",
    "\n",
    "### AI\n",
    "AlphaZero is a neural network AI that has beaten world champions in [Chess](https://www.chess.com/news/view/updated-alphazero-crushes-stockfish-in-new-1-000-game-match), [Go](https://www.theverge.com/2019/11/27/20985260/ai-go-alphago-lee-se-dol-retired-deepmind-defeat), and [Starcraft](https://www.theverge.com/2019/10/30/20939147/deepmind-google-alphastar-starcraft-2-research-grandmaster-level)\n",
    "\n",
    "[OpenAI](https://openai.com/blog/solving-rubiks-cube/) has also applied neural networks to solving physical robotics problems.\n",
    "\n",
    "<img src=\"img/rubiks.jpg\" width=600 />\n",
    "\n",
    "### Other applications\n",
    "\n",
    "Many machine learning techniques can also be implemented through neural networks (linear regression, logistic regression, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Structure of a Neural Network\n",
    "\n",
    "Black Box view:\n",
    "<img src=\"img/black_box.png\" width=600 />\n",
    "Layer view:\n",
    "<img src=\"img/layers.png\" width=600 />\n",
    "\n",
    "Neuron view:\n",
    "<img src=\"img/layers_with_neurons.png\" width=600 />\n",
    "\n",
    "As functions:\n",
    "$$\\mathbf{y} = \\mathbf{f}_y(\\mathbf{W}_4, \\mathbf{b}_4, \\mathbf{h}_3)$$\n",
    "$$\\mathbf{h}_3 = \\mathbf{f}_{h(3)}(\\mathbf{W}_3, \\mathbf{b}_3, \\mathbf{h}_2)$$\n",
    "$$\\mathbf{h}_2 = \\mathbf{f}_{h(2)}(\\mathbf{W}_2, \\mathbf{b}_2, \\mathbf{h}_1)$$\n",
    "$$\\mathbf{h}_1 = \\mathbf{f}_{h(1)}(\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{x})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Forward propagation and backpropagation\n",
    "\n",
    "## Forward propagation: \n",
    "1. Calculate first hidden layer from inputs\n",
    "2. Calculate second hidden layer from first hidden layer\n",
    "3. etc.\n",
    "4. Calculate outputs from last hidden layer\n",
    "\n",
    "\n",
    "In terms of the functions above,\n",
    "1. $$\\mathbf{h}_1 = \\mathbf{f}_{h(1)}(\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{x})$$\n",
    "2. $$\\mathbf{h}_2 = \\mathbf{f}_{h(2)}(\\mathbf{W}_2, \\mathbf{b}_2, \\mathbf{h}_1)$$\n",
    "3. $$\\mathbf{h}_3 = \\mathbf{f}_{h(3)}(\\mathbf{W}_3, \\mathbf{b}_3, \\mathbf{h}_2)$$\n",
    "4. $$\\mathbf{y} = \\mathbf{f}_y(\\mathbf{W}_4, \\mathbf{b}_4, \\mathbf{h}_3)$$\n",
    "\n",
    "\n",
    "And we can calculate the value of a cost function:\n",
    "$$C(\\mathbf{y}, \\mathbf{y}_{obs})$$\n",
    "\n",
    "## Backpropagation \n",
    "Backpropagation is how we find the gradient across all weights so that we can do gradient descent. \n",
    "\n",
    "Review:\n",
    "- Chain Rule\n",
    "\n",
    "$$ z =  f(g(x)) $$\n",
    "$$ u = g(x) $$\n",
    "$$ \\frac{dz}{dx} =  \\frac{dz}{du} \\frac{du}{dx}$$\n",
    "\n",
    "For example:\n",
    "$$ \\frac{dC}{dW_3} = \\frac{dC}{d\\mathbf{y}}\\frac{d\\mathbf{y}}{d\\mathbf{h}_3}\\frac{d\\mathbf{h}_3}{d\\mathbf{W}_3} $$\n",
    "\n",
    "In general:\n",
    "$$ \n",
    "\\frac{dC}{dW_i} = \n",
    "    \\frac{dC}{d\\mathbf{y}}\n",
    "    \\frac{d\\mathbf{y}}{d\\mathbf{h}_m}\n",
    "    \\frac{d\\mathbf{h}_m}{d\\mathbf{h}_{m-1}} ... \n",
    "    \\frac{d\\mathbf{h}_{i+1}}{d\\mathbf{h}_{i}}\n",
    "    \\frac{d\\mathbf{h}_{i}}{d\\mathbf{W}_{i}}\n",
    "$$\n",
    "\n",
    "Or more compactly, \n",
    "$$   \\frac{dC}{\\mathbf{dW}_i} = \\frac{dC}{\\mathbf{dh}_{i}}\\frac{\\mathbf{dh}_{i}}{\\mathbf{dW}_i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building a simple artificial neuron\n",
    "\n",
    "\n",
    "[<img src=\"img/neuron.jpg\" width=400 />](https://medium.com/@jayeshbahire/the-artificial-neural-networks-handbook-part-4-d2087d1f583e)\n",
    "\n",
    "\n",
    "We will use the Rectified Linear Unit or [ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) as our activation function: \n",
    "\n",
    "$$    ReLU(x) = \\Bigg\\{\n",
    "        \\begin{array}{ll}\n",
    "        x, & \\text{if } x > 0\\\\\n",
    "        0, & \\text{otherwise }\\\\\n",
    "        \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import demo\n",
    "\n",
    "inputs = demo.make_inputs(5)\n",
    "weights, bias = demo.initialize_neuron(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Params\n",
    "    ———-\n",
    "    x: float\n",
    "    \n",
    "    Returns\n",
    "    ———-\n",
    "    output: float\n",
    "    '''\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this together\n",
    "def neuron(prev_layer, weights, bias, activation_function):\n",
    "    '''\n",
    "    Params\n",
    "    ———-\n",
    "    prev_layer: numpy.ndarray\n",
    "        Should have shape (m, ). Contains output values from the previous layer.\n",
    "    weights: numpy.ndarray \n",
    "        Should have shape (m, ). Contains weights connecting previous layer to this neuron\n",
    "    bias: float\n",
    "    activation_function: function(float)\n",
    "    \n",
    "    Returns\n",
    "    ———-\n",
    "    output: float\n",
    "        the output of the neuron\n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backpropagation, we'll also need a way to get derivatives across a neuron. \n",
    "\n",
    "$$ \\varphi(z) = \\Bigg\\{\n",
    "        \\begin{array}{ll}\n",
    "        z, & \\text{if } z > 0\\\\\n",
    "        0, & \\text{otherwise }\\\\\n",
    "        \\end{array} $$\n",
    "$$ z = w_1x_1 + w_2x_2 + ... + w_mx_m + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Ask about vanishing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_relu(x):\n",
    "    '''\n",
    "    Params\n",
    "    ———-\n",
    "    x: float\n",
    "    \n",
    "    Returns\n",
    "    ———-\n",
    "    output: float\n",
    "    '''\n",
    "\n",
    "    \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_neuron(prev_layer, weights, bias, neuron_output, d_activation):\n",
    "    '''\n",
    "    Params\n",
    "    ———-\n",
    "    prev_layer: numpy.ndarray\n",
    "        Should have shape (m, ). Contains output values from the previous layer.\n",
    "    weights: numpy.ndarray \n",
    "        Should have shape (m, ). Contains weights connecting previous layer to this neuron\n",
    "    bias: float\n",
    "    activation_function: function(float)\n",
    "    neuron_output: float\n",
    "        This is the output of this neuron from forward propagation\n",
    "    d_activation: function\n",
    "        the derivative of the activation function\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    ———-\n",
    "    output: d_dweights: numpy.ndarray\n",
    "        derivative of the neuron with respect to weights\n",
    "    output: d_dlayer: numpy.ndarray\n",
    "        derivative of the neuron with respect to previous layer\n",
    "    output: d_dbias: numpy.ndarray\n",
    "        derivative of the neuron with respect to bias\n",
    "    '''\n",
    "\n",
    "    \n",
    "    return d_dweights, d_dlayer, d_dbias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the neuron function. (inputs, weights, bias are imported from the demo script)\n",
    "inputs = demo.make_inputs(5)\n",
    "weights, bias = demo.initialize_neuron(5)\n",
    "\n",
    "\n",
    "print(\"inputs: {}\".format(inputs))\n",
    "print(\"weights: {}\".format(weights))\n",
    "print(\"bias: {}\".format(bias))\n",
    "print(\"output: {}\".format(neuron(inputs, weights, bias, relu)))\n",
    "\n",
    "neuron_output = neuron(inputs, weights, bias, relu)\n",
    "d_dweights, d_dlayer, d_dbias = derivative_neuron(inputs, weights, bias, neuron_output, d_relu)\n",
    "\n",
    "print(\"\")\n",
    "print(\"d/dw: {}\".format(d_dweights))\n",
    "print(\"d/dl: {}\".format(d_dlayer))\n",
    "print(\"d/db: {}\".format(d_dbias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recognizing Handwritten Digits\n",
    "\n",
    "Just use Keras!\n",
    "\n",
    "Example from https://www.tensorflow.org/datasets/keras_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "train, test = demo.mnist_data()\n",
    "\n",
    "plt.matshow(next(tfds.as_numpy(train))[0][0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Flatten())\n",
    "# hidden layers\n",
    "\n",
    "# output layer\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train,\n",
    "    epochs=6,\n",
    "    validation_data=test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine predictions\n",
    "batch_index = 0\n",
    "\n",
    "example = next(tfds.as_numpy(test))[0]\n",
    "print(\"Network thinks this image is a {}\".format(np.argmax(model.predict(example)[batch_index])))\n",
    "plt.matshow(example[batch_index,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else would we do in a full day lesson?\n",
    "\n",
    "- Implement the training of a neural network using numpy.\n",
    "- Go over all commonly used activation functions and discuss the pros/cons of each. \n",
    "- Understand what vanishing gradient means, and how that applies to choice of activation function\n",
    "- Go over commonly used cost functions\n",
    "- Learn how GPUs can be used to accelerate the training of neural nets.\n",
    "- Overview of convolutional layers, GRUs and LSTMs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural]",
   "language": "python",
   "name": "conda-env-neural-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
