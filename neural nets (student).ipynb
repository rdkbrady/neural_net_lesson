{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Applications of Neural Networks\n",
    "\n",
    "\n",
    "### Text Generation\n",
    "The [GPT-2](http://jalammar.github.io/illustrated-gpt2/) model is a **decoder** model which uses previous words to predict the next one in a sequence. You can experiment with a limited version of this model at [talk to transformer](https://talktotransformer.com/). \n",
    "\n",
    "**Input:** In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    "**Output:** The amazing find was made by researchers of the Universidad Austral de Chile, thanks to one of the country's most highly active amateur scientists. Of course, the creature in question was not a real unicorn, but a llama, and unfortunately the strange creature was killed before researchers could analyze its vocalization.\n",
    "\n",
    "\n",
    "### Image Recognition\n",
    "The [VGG-16](https://neurohive.io/en/popular-networks/vgg16/) model is a **deep convolutional** network that can classify image regardless of the scale or rotation of the objects in the image.\n",
    "\n",
    "doge? | doge?\n",
    "- | - \n",
    "[<img src=\"img/shibe.jpg\" width=240 />](https://knowyourmeme.com/memes/doge) |[<img src=\"img/doge.jpg\" width=240 />](https://en.wikipedia.org/wiki/Doge_of_Venice)\n",
    "\n",
    "\n",
    "### Image Generation\n",
    "[Generative adversarial networks](https://neurohive.io/en/news/deepfake-videos-gan-sythesizes-a-video-from-a-single-photo/]) can be used to create plausible images and video.\n",
    "<img src=\"img/gan.png\" width=600 />\n",
    "\n",
    "### AI\n",
    "AlphaZero is a neural network AI that has beaten world champions in [Chess](https://www.chess.com/news/view/updated-alphazero-crushes-stockfish-in-new-1-000-game-match), [Go](https://www.theverge.com/2019/11/27/20985260/ai-go-alphago-lee-se-dol-retired-deepmind-defeat), and [Starcraft](https://www.theverge.com/2019/10/30/20939147/deepmind-google-alphastar-starcraft-2-research-grandmaster-level)\n",
    "\n",
    "[OpenAI](https://openai.com/blog/solving-rubiks-cube/) has also applied neural networks to solving physical robotics problems.\n",
    "\n",
    "<img src=\"img/rubiks.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's build a simple neuron\n",
    "\n",
    "[<img src=\"img/feedforward.png\" width=200 />](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)\n",
    "\n",
    "\n",
    "[<img src=\"img/neuron.jpg\" width=400 />](https://medium.com/@jayeshbahire/the-artificial-neural-networks-handbook-part-4-d2087d1f583e)\n",
    "\n",
    "A neuron is a function that takes a weighted sum of values and applies some transformation. In this case, the values in the weighted sum will be all of the outputs from the previous layer. \n",
    "\n",
    "$$y=\\varphi\\big(\\sum_{j=0}^m w_{j}x_j + b\\big)$$\n",
    "\n",
    "Indices:\n",
    "- $j$ is an index for a neuron in the previous layer\n",
    "- $m$ is the number of neurons in the previous layer\n",
    "\n",
    "Variables:\n",
    "- $y$: the output of our neuron.\n",
    "- $w_{j}$: the weight connecting $x_j$ in the previous layer and the current neuron\n",
    "- $x_j$ the output of a neuron in the previous layer.\n",
    "- $b$ is the 'bias'.\n",
    "\n",
    "Functions:\n",
    "- $\\varphi$: a transformation that we call an 'activation function'. The simplest would be $\\phi(x) = x$, the identity function. \n",
    "\n",
    "We will uses the Rectified Linear Unit or [ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) as our activation function: \n",
    "\n",
    "$$    ReLU(x) = \\Bigg\\{\n",
    "        \\begin{array}{ll}\n",
    "        x, & \\text{if } x > 0\\\\\n",
    "        0, & \\text{otherwise }\\\\\n",
    "        \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create some values to supply to our neuron function\n",
    "random.seed(1234)\n",
    "prev_layer_size = 10\n",
    "prev_layer = random.random(prev_layer_size)\n",
    "prev_layer = np.append(prev_layer, [1])\n",
    "weights = random.random(prev_layer_size + 1)\n",
    "bias = random.uniform()\n",
    "\n",
    "relu = lambda x: x if x > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "def neuron(prev_layer, weights, bias, activation_function):\n",
    "    '''\n",
    "    prev_layer: numpy ndarray with shape (m, ) of output values from the previous layer\n",
    "    weights: numpy ndarray with shape (m, ) of weights connecting previous layer to this neuron\n",
    "    activation_function: a function which takes a float as an argument and returns a new float\n",
    "    '''\n",
    "    # Write this together    \n",
    "    \n",
    "    return neuron_output\n",
    "    \n",
    "neuron(prev_layer, weights, bias, relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let's build a network\n",
    "\n",
    "[<img src=\"img/feedforward.png\" width=200 />](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)\n",
    "\n",
    "We're going to build a simple neural network with three layers - input, hidden, and output\n",
    "\n",
    "[<img src=\"img/weights_matrix.png\" width=600 />](https://www.jeremyjordan.me/intro-to-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "input_size = 2\n",
    "hidden_layer_size = 2\n",
    "output_size = 1\n",
    "\n",
    "# create random input data\n",
    "input_layer = random.random(input_size)\n",
    "\n",
    "# initialize weights and bias for the hidden layer\n",
    "weights_matrix_input_hidden = random.random(input_size * hidden_layer_size).reshape(input_size, hidden_layer_size) - 0.5\n",
    "bias_vector_input_hidden = random.random(hidden_layer_size) - 0.5\n",
    "\n",
    "# initialize weights and bias for the output layer\n",
    "weights_matrix_hidden_output = random.random(hidden_layer_size * output_size).reshape(hidden_layer_size, output_size) - 0.5\n",
    "bias_vector_hidden_output = random.random(output_size) - 0.5\n",
    "\n",
    "# vectorize the activation function so we can apply it across all neurons in a layer\n",
    "v_relu = np.vectorize(relu)\n",
    "\n",
    "\n",
    "def neuron_layer(prev_layer, weights_matrix, bias_vector, vectorized_activation_function):\n",
    "    '''\n",
    "    prev_layer: numpy ndarray with shape (m, ) of the m output values from the previous layer\n",
    "    weights: numpy ndarray with shape (m, n) of weights connecting previous layer to all n neurons in this layer\n",
    "    vectorized_activation_function: a vectorized (np.vectorize) function which takes a vector of float as an argument \n",
    "        and returns a new vector of floats\n",
    "    '''\n",
    "    \n",
    "    # write this together    \n",
    "    \n",
    "    return layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = neuron_layer(input_layer, weights_matrix_input_hidden, bias_vector_input_hidden, v_relu)\n",
    "output_layer = neuron_layer(hidden_layer, weights_matrix_hidden_output, bias_vector_hidden_output, v_relu)\n",
    "print(\"Inputs in input layer: {}\".format(input_layer))\n",
    "print(\"Activations from hidden layer: {}\".format(hidden_layer))\n",
    "print(\"Activations from output layer: {}\".format(output_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How good are the predictions?\n",
    "\n",
    "The output layer gives us a predicted value for our response, given our inputs. We can assess how good this response is using a cost function. One cost function is the squared error:\n",
    "\n",
    "$$ C(\\hat{y}) = (y - \\hat{y})^2$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_square_error(yhat, y):\n",
    "    square_error = (y - yhat)**2\n",
    "    return(square_error)\n",
    "\n",
    "yhat = 3.4\n",
    "y = 3.1\n",
    "\n",
    "\n",
    "print(\"C(yhat, y) at yhat = {}: {}\".format(yhat, cost_square_error(yhat, y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How will changing parts of the network affect the cost function?\n",
    "\n",
    "<img src=\"img/full_net.png\" width=400 />\n",
    "\n",
    "\n",
    "##### How would a change in $\\hat{y}$ affect $C(\\hat{y})$?\n",
    "\n",
    "<img src=\"img/y_to_C.png\" width=200 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the shape of the cost function C:\n",
    "plot_range = np.arange(0, 5, 0.1)\n",
    "plt.plot(plot_range, [cost_square_error(x, 3) for x in plot_range])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see how a change in $\\hat{y}$ affects $C(\\hat{y})$ by taking the derivative of C with respect to $\\hat{y}$:\n",
    "\n",
    "$$\\frac{dC}{d\\hat{y}} = -2(y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_cost_square_error(yhat, y):\n",
    "    d_sq_error = -2 * (y - yhat)\n",
    "    return(d_sq_error)\n",
    "\n",
    "print(\"dC/dyhat: {}\".format(derivative_cost_square_error(yhat, y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### What effect will a change in a weight from hidden layer to output layer have on the output layer?\n",
    "\n",
    "<img src=\"img/h_to_y.png\" width=200 />\n",
    "\n",
    "$$    \\frac{\\partial\\hat{y}}{\\partial v} = \\Bigg\\{\n",
    "        \\begin{array}{ll}\n",
    "        h, & \\text{if } vh > 0\\\\\n",
    "        0, & \\text{otherwise }\\\\\n",
    "        \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(value):\n",
    "    if(value > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def layer_derivative_weight(layer_value, previous_layer_value):\n",
    "    return relu_derivative(layer_value) * previous_layer_value\n",
    "    \n",
    "v_layer_derivative_weights = np.vectorize(layer_derivative_weight, excluded = \"previous_layer_value\")\n",
    "\n",
    "print(\"dyhat / dv: {}\".format(v_layer_derivative_weights(output_layer, hidden_layer)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### What effect will a change in a hidden layer activation to output layer have on the output layer?\n",
    "\n",
    "<img src=\"img/h_to_y.png\" width=200 />\n",
    "\n",
    "$$    \\frac{\\partial\\hat{y}}{\\partial h} = \\Bigg\\{\n",
    "        \\begin{array}{ll}\n",
    "        v, & \\text{if } vh > 0\\\\\n",
    "        0, & \\text{otherwise }\\\\\n",
    "        \\end{array}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_derivative_prev_layer(layer_value, weights):\n",
    "    return relu_derivative(layer_value) * weights\n",
    "\n",
    "    \n",
    "v_layer_derivative_prev_layer = np.vectorize(layer_derivative_prev_layer, excluded = \"weights\")    \n",
    "    \n",
    "print(\"dyhat / dv: \\n{}\".format(v_layer_derivative_prev_layer(output_layer, weights_matrix_hidden_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What effect will a change in a weight from hidden layer to output layer have on the cost function?\n",
    "\n",
    "<img src=\"img/h_to_C.png\" width=400 />\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial v} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial\\hat{y}}{\\partial v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_cost_square_error(yhat, y) * v_layer_derivative_weights(output_layer, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What effect will a change in a hidden layer activation have on the cost function?\n",
    "\n",
    "<img src=\"img/h_to_C.png\" width=400 />\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial h} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial\\hat{y}}{\\partial h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_cost_square_error(yhat, y) * v_layer_derivative_prev_layer(output_layer, weights_matrix_hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What effect will a change in a weight from input layer to hidden layer have on the hidden layer?\n",
    "<img src=\"img/x_to_h.png\" width=200 />\n",
    "\n",
    "$$    \\frac{\\partial h}{\\partial w} = \\Bigg\\{\n",
    "        \\begin{array}{ll}\n",
    "        x, & \\text{if } wx > 0\\\\\n",
    "        0, & \\text{otherwise }\\\\\n",
    "        \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_layer_derivative_weights(hidden_layer, input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What effect will a change in a weight from input layer to hidden layer have on the output layer?\n",
    "<img src=\"img/x_to_y.png\" width=400 />\n",
    "\n",
    "$$    \\frac{\\partial\\hat{y}}{\\partial w} = \\frac{\\partial \\hat{y}}{\\partial h} \\frac{\\partial h}{\\partial w}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " v_layer_derivative_prev_layer(output_layer, weights_matrix_hidden_output) * v_layer_derivative_weights(hidden_layer, input_layer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What effect will a change in a weight from input layer to hidden layer have on the cost function?\n",
    "\n",
    "<img src=\"img/full_net.png\" width=600 />\n",
    "\n",
    "$$    \\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h} \\frac{\\partial h}{\\partial w}\n",
    "$$\n",
    "\n",
    "Note that we already calculated $\\frac{\\partial C}{\\partial h} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h}$ above!\n",
    "\n",
    "$$    \\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial h} \\frac{\\partial h}{\\partial w} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_cost_square_error(yhat, y) * v_layer_derivative_prev_layer(output_layer, weights_matrix_hidden_output) * v_layer_derivative_weights(hidden_layer, input_layer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Using Gradient Descent to Improve the Prediction\n",
    "\n",
    "We'd like to make the cost function C smaller. Since we now know how small changes in each weight will effect C, we can iteratively make those changes to minimize C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Doing this the easy way\n",
    "\n",
    "Just use Keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2)) # add a hidden layer of size 2, connected to the input layer of size 2\n",
    "model.add(Activation('relu')) # use relu activation\n",
    "model.add(Dense(1)) # add the output layer of size 1, connected to the hidden layer\n",
    "model.add(Activation('relu')) # use relu activation\n",
    "\n",
    "# compile the model using a cost function of mean squared error and stochastic gradient descent\n",
    "model.compile(optimizer='sgd',  loss='mse') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random.random(300).reshape(-1, 2) - 0.5\n",
    "y = 0.4 * X[:,0] + 2.1 * X[:,1]\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stonks]",
   "language": "python",
   "name": "conda-env-stonks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
